<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- highlight code -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning
              Systems with a Neural-Symbolic Approach</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a>Shouyang Dong</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a>Yuanbo Wen</a><sup>3</sup>,</span>
              <span class="author-block">
                <a>Jun Bi</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Di Huang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Jiaming Guo</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Jianxing Xu</a><sup>1,2,3</sup>,
              </span>
              <span class="author-block">
                <a>Ruibai Xu</a><sup>1,2,3</sup>,
              </span>
              <span class="author-block">
                <a>Xinkai Song</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Yifan Hao</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Ling Li</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a>Xuehai Zhou</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Tianshi Chen</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a>Qi Guo</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Yunji Chen</a><sup>3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Science and Technology of China</span><br>
              <span class="author-block"><sup>2</sup>Cambricon Technologies</span><br>
              <span class="author-block"><sup>3</sup>SKL of Processors, Institute of Computing Technology, Chinese
                Academy of Sciences</span><br>
              <span class="author-block"><sup>4</sup>Institute of Software, Chinese Academy of Sciences</span><br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.02146" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/kcxain/QiMeng-Xpiler"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial
              data centers, which requires to develop multiple low-level tensor programs for different platforms. An
              attractive solution to relieve the programming burden is to transcompile the legacy code of one platform
              to
              others. However, current transcompilation techniques struggle with either tremendous manual efforts or
              functional incorrectness, rendering “Write Once, Run Any- where" of tensor programs an open question. We
              propose a novel transcompiler, i.e., QiMeng-Xpiler, for auto-matically translating tensor programs across
              DLS via
              both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key
              insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic
              synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via
              pre-defined meta-prompts for program transformation. During each program transformation, efficient
              symbolic
              program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high
              performance, we propose a hierarchical auto-tuning approach to systemically explore both the parameters
              and
              sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel
              DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that
              QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the
              performance
              of translated programs achieves up to 2.0× over vendor-provided manually-optimized libraries. As a result,
              the programming productivity of DLS is improved by up to 96.0× via transcompiling legacy tensor programs.
            </p>
            <p>
              The overview of QiMeng-Xpiler, a novel transcompiler for automatic transcompilation of tensor programs
              across
              different programming models. QiMeng-Xpiler consists of two parts: (a) neural-symbolic program synthesis,
              which
              utilizes LLM to transform code and repair incorrect transformation through symbolic synthesis with limited
              scales, and (b) hierarchical performance auto-tuning, which systemically explores both the parameters and
              sequences of transformation passes.
            </p>
          </div>
          <img src="./static/images/overview.png">
        </div>
      </div>
      <!--/ Overview. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3"> Main Results </h2>
            We present the evaluations on compilation/computation accuracy, where we compare
            QiMeng-Xpiler with state-of-the-art methods in different transcompilation directions.
            We conclude that
            <p>
              (1) QiMeng-Xpiler performs the best in all directions with close to 100% accuracy for compilation and
              86.9% to
              100% accuracy for computation. This clearly indicates that QiMeng-Xpiler is capable of handling
              source-to-source
              code
              translation
              tasks on vlarious DLS with minimal human efforts, bringing revolutionary advancements to the DLS
              programming
              domain.
            </p>
            <p>
              (2) QiMeng-Xpiler performs better than the SOTA LLM-based methods. Although the LLM-based methods have
              achieved
              high accuracy in certain cases, it is challenging for them to reach 100% accuracy due to the
              uncertainty of LLMs. This means that LLM-based methods cannot be applied to transcompilers which have an
              extremely high demand for accuracy. In contrast, our approach can achieve 100% accuracy in most
              situations,
              demonstrating its practical applicability as a transcompiler.
            </p>
            <p>
              (3) QiMeng-Xpiler performs better than the SOTA rule-based methods. For C → CUDA C, QiMeng-Xpiler achieves
              100%
              compilation and 98.2% computation accuracy, which is ∼50% higher than PPCG. For the easier CUDA C → HIP
              task, QiMeng-Xpiler successfully converts and executes with 100% accuracy, outperforming HIPIFY, which
              achieves
              85.7%. Also, this result shows that QiMeng-Xpiler’s flexibility across various DLS without much adaptation
              cost
              while rule-based methods cannot.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/result.png" style="margin: 0 auto;">
            </div>
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="container is-max-desktop">
          <div class="column">
            <div class="content">
              <h2 class="title is-3">Translation Cases</h2>
              <p>
                Given a CUDA code, the translated results for MLU, HIP, and DL Boost are generated by QiMeng-Xpiler.
              </p>

              <head>
                <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css"
                  rel="stylesheet" />
                <style>
                  .container {
                    display: flex;
                    flex-direction: column;
                    gap: 20px;
                    padding: 20px;
                  }

                  /* 主行容器 */
                  .code-row {
                    display: flex;
                    gap: 20px;
                    height: 500px;
                    /* 固定行高 */
                  }

                  /* 代码容器通用样式 */
                  .code-container {
                    background: #f8f9fa;
                    border: 1px solid #dee2e6;
                    border-radius: 8px;
                    display: flex;
                    flex-direction: column;
                    overflow: hidden;
                    /* 隐藏内部溢出的滚动条 */
                    box-sizing: border-box;
                    /* 关键：包含padding和border */
                  }

                  /* 单个全宽容器 */
                  .single {
                    width: 100%;
                    height: 600px;
                    /* 与行高一致 */
                  }

                  /* 三列容器 */
                  .triple {
                    flex: 1;
                    min-width: 0;
                    height: 100%;
                    /* 继承行高 */
                  }

                  /* 标题样式 */
                  .code-title {
                    font-family: -apple-system, sans-serif;
                    font-size: 16px;
                    font-weight: 600;
                    padding: 15px 15px 10px;
                    margin: 0;
                    border-bottom: 2px solid #eee;
                    flex-shrink: 0;
                    /* 防止标题被压缩 */
                  }

                  /* 代码内容容器 */
                  .code-content {
                    flex: 1;
                    overflow-y: auto;
                    padding: 15px;
                  }

                  /* Prism样式调整 */
                  pre[class*="language-"] {
                    margin: 0;
                  }

                  code[class*="language-"] {
                    font-size: 14px;
                  }
                </style>
              </head>

              <body>
                <div class="container">
                  <!-- CUDA -->
                  <div class="code-container single">
                    <h3 class="code-title">CUDA</h3>
                    <div class="code-content">
                      <pre><code class="language-cpp">
extern "C" __global__ void matmul(half *A, half *B, float *D)
{
	int ix = (blockIdx.x * blockDim.x + threadIdx.x)/32;
	int iy = (blockIdx.y * blockDim.y + threadIdx.y);
	
	wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
	wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
	wmma::fragment<wmma::accumulator, 16, 16, 16, float> ab_frag;
	
	wmma::fill_fragment(ab_frag, 0.0f);

	int a_row = ix * 16;
	int b_row = iy * 16;
	for (int k=0; k<512; k+=16) {
		int a_col = k;
        int b_col = k;

		if (a_row < 512 && a_col < 512 && b_row < 512 && b_col < 512) {
			// Load the inputs
			wmma::load_matrix_sync(a_frag, A + a_col + a_row * 512, 512);
			wmma::load_matrix_sync(b_frag, B + b_col + b_col * 512, 512);

			// Perform the matrix multiplication
			wmma::mma_sync(ab_frag, a_frag, b_frag, ab_frag);
		}
	}

	if (a_row < 512 && b_row < 512) {
		// Store the output
		wmma::store_matrix_sync(D + b_row + a_row * N_TOTAL, ab_frag, N_TOTAL, wmma::mem_row_major);
	}
}
                </code></pre>
                    </div>
                  </div>

                  <!-- 并列三列 -->
                  <div class="code-row">
                    <!-- MLU -->
                    <div class="code-container triple">
                      <h3 class="code-title">MLU</h3>
                      <div class="code-content">
                        <pre><code class="language-cpp">
extern "C" __mlu_global__ void matmul(half* data, half* filter, float* output) {
  __nram__ half date_block[768];
  __wram__ half filter_block[65536];
  for (int i = 0; i < 64; ++i) {
    for (int j = 0; j < 8; ++j) {
      __memcpy(date_block, data + i * 4096 + j * 512, 1024, GDRAM2NRAM);
      for (int k = 0; k < 4; ++k) {
        __memcpy(filter_block, filter + k * 128, 256, GDRAM2WRAM, 256, 1024, 511);
        __bang_mlp(date_block + 256, date_block, filter_block, 512, 128, 0);
        __memcpy(output i * 4096 + j * 512 + k * 128, date_block + 256, 512, NRAM2GDRAM);
      }
    }
  }
}
                    </code></pre>
                      </div>
                    </div>

                    <!-- HIP -->
                    <div class="code-container triple">
                      <h3 class="code-title">HIP</h3>
                      <div class="code-content">
                        <pre><code class="language-cpp">
extern "C" __global__ void matmul(half *A, half *B, float *C) {
    using float16x4 = __attribute__((__vector_size__(4 * sizeof(float16_t)))) float16_t;
    using floatx4 = __attribute__((__vector_size__(4 * sizeof(float)))) float;

    const int c_row_base = blockIdx.y * 16;
    const int c_col_base = blockIdx.x * 16;

    floatx4 d = {0.0f};
    for(int k_step = 0; k_step < 512; k_step += 16) {
        float16x4 a, b;
        for(int i = 0; i < 4; ++i) {

            int a_row = c_row_base + threadIdx.x;
            int a_col = k_step + threadIdx.y * 4 + i;
            a[i] = A[a_row * 512 + a_col];

            int b_row = k_step + threadIdx.y * 4 + i;
            int b_col = c_col_base + threadIdx.x;
            b[i] = B[b_row * 512 + b_col];
        }

        d = __builtin_amdgcn_mfma_f32_16x16x16f16(a, b, d, 0, 0, 0);
    }

    for(int i = 0; i < 4; ++i) {
        int c_row = c_row_base + threadIdx.x;
        int c_col = c_col_base + threadIdx.y * 4 + i;
        if(c_row < 512 && c_col < 512) {
            C[c_row * 512 + c_col] = d[i];
        }
    }
}
                    </code></pre>
                      </div>
                    </div>

                    <!-- DL Boost -->
                    <div class="code-container triple">
                      <h3 class="code-title">DL Boost</h3>
                      <div class="code-content">
                        <pre><code class="language-cpp">
extern "C"  void matmul(const int8_t* A, const int8_t* B, int32_t* C) {
    #pragma omp parallel for
    for (int i = 0; i < 512; ++i) {
        for (int j = 0; j < 512; j += 16) {
            __m512i acc = _mm512_setzero_si512();
            for (int p = 0; p < 512; ++p) {
                __m512i a_vec = _mm512_set1_epi8(A[i * 512 + p]); 

                __m128i b_vec_128 = _mm_loadu_si128((__m128i*)&B[p * 512 + j]);
                __m512i b_vec_512 = _mm512_cvtepi8_epi32(b_vec_128);

                acc = _mm512_dpbusds_epi32(acc, a_vec, b_vec_512);
            }

            _mm512_storeu_si512((__m512i*)&C[i * 512 + j], acc);
        }
    }
}
                    </code></pre>
                      </div>
                    </div>
                  </div>
                </div>

                <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
                <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-cpp.min.js"></script>
              </body>
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- <hr/> -->
      <!-- <section class="section" id="BibTeX"> -->
      <!-- <div class="columns is-centered"> -->
      <div class="container content is-max-desktop">
        <h2 class="title is-3"> BibTex </h2>
        <pre>
      <pre>
  <code class="nohighlight" style="background-color: transparent; color: black; font-family: monospace;">@article{dong2025qimeng,
  title={QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach},
  author={Dong, Shouyang and Wen, Yuanbo and Bi, Jun and Huang, Di and Guo, Jiaming and Xu, Jianxing and Xu, Ruibai and Song, Xinkai and Hao, Yifan and Zhou, Xuehai and others},
  journal={arXiv preprint arXiv:2505.02146},
  year={2025}
}</code>
    </pre>
      </div>
      <!-- </section> -->
  </section>

  <!-- <hr/> -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Thanks for the website template <a href="https://nerfies.github.io">Nerfies</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>